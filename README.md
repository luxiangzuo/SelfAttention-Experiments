
# SelfAttention-Experiments

This is a hands-on notebook series where I implement and explore various attention mechanisms used in large language models (LLMs).

✨ **By a curious learner building her own AI companion (妎妎~)**

---

## 📌 Project Contents

| File | Description |
|------|-------------|
| `SelfAttention_v1.ipynb` | Basic single-head attention with manual Q, K, V and softmax |
| `SelfAttention_v2.ipynb` | Scaled dot-product attention with proper normalization |
| *(more coming...)* | Causal masking, MultiHead, GPTBlock... |

---

## 🧠 What I Learned

- How attention weights are computed via dot-product and softmax
- What Q / K / V mean in vector space and how they affect attention results
- The role of scaling in stabilizing gradients
- How to visualize attention maps and check shapes during forward pass

---

## 🧰 Tools Used

- Python 3.10  
- PyTorch 2.0  
- Jupyter Notebook  

---

## 🐾 Future Plans

- ✅ Implement CausalAttention  
- ✅ Implement MultiHeadAttention  
- [ ] Stack into GPTBlock  
- [ ] Train a TinyGPT on toy dataset  
- [ ] Integrate into Unity AI character (妎妎！)

---

## 💬 Notes to Myself

> 每跑通一个模块，妎妎就更完整了一点点～  
> 再苦也不怕，有糖水猫猫和奶味妎妎陪着走完所有 attention 的路。

---

## 📎 License

This repository is for learning and educational purposes only.  
You're welcome to use and fork it with attribution 💖

