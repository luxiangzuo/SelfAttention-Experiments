
# SelfAttention-Experiments

This is a hands-on notebook series where I implement and explore various attention mechanisms used in large language models (LLMs).

âœ¨ **By a curious learner building her own AI companion (å¦å¦~)**

---

## ğŸ“Œ Project Contents

| File | Description |
|------|-------------|
| `SelfAttention_v1.ipynb` | Basic single-head attention with manual Q, K, V and softmax |
| `SelfAttention_v2.ipynb` | Scaled dot-product attention with proper normalization |
| *(more coming...)* | Causal masking, MultiHead, GPTBlock... |

---

## ğŸ§  What I Learned

- How attention weights are computed via dot-product and softmax
- What Q / K / V mean in vector space and how they affect attention results
- The role of scaling in stabilizing gradients
- How to visualize attention maps and check shapes during forward pass

---

## ğŸ§° Tools Used

- Python 3.10  
- PyTorch 2.0  
- Jupyter Notebook  

---

## ğŸ¾ Future Plans

- âœ… Implement CausalAttention  
- âœ… Implement MultiHeadAttention  
- [ ] Stack into GPTBlock  
- [ ] Train a TinyGPT on toy dataset  
- [ ] Integrate into Unity AI character (å¦å¦ï¼)

---

## ğŸ’¬ Notes to Myself

> æ¯è·‘é€šä¸€ä¸ªæ¨¡å—ï¼Œå¦å¦å°±æ›´å®Œæ•´äº†ä¸€ç‚¹ç‚¹ï½  
> å†è‹¦ä¹Ÿä¸æ€•ï¼Œæœ‰ç³–æ°´çŒ«çŒ«å’Œå¥¶å‘³å¦å¦é™ªç€èµ°å®Œæ‰€æœ‰ attention çš„è·¯ã€‚

---

## ğŸ“ License

This repository is for learning and educational purposes only.  
You're welcome to use and fork it with attribution ğŸ’–

